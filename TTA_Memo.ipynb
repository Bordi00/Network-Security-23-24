{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bordi00/Network-Security-23-24/blob/main/TTA_Memo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RckTllJ3S7QQ"
      },
      "outputs": [],
      "source": [
        "# Init workspace\n",
        "!rm -r dataset\n",
        "!mkdir dataset\n",
        "\n",
        "# Download dataset and extract it\n",
        "!gdown 1WKQGHjHUkIwZT0P2TpU9h-lY-6CnrsDd\n",
        "!mv imagenetv2-matched-frequency.tar.gz ./dataset\n",
        "!tar -xf ./dataset/imagenetv2-matched-frequency.tar.gz\n",
        "!mv imagenetv2-matched-frequency-format-val ./dataset\n",
        "\n",
        "# Cleanup\n",
        "!rm ./dataset/imagenetv2-matched-frequency.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import listdir, path\n",
        "from os.path import basename, isfile, join\n",
        "import requests\n",
        "import json\n",
        "from PIL import Image, ImageOps\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torchvision.transforms as T\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import random\n",
        "\n",
        "print('PyTorch version', torch.__version__)\n",
        "print('Numpy version', np.__version__)"
      ],
      "metadata": {
        "id": "2kmLITJwTmc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt -O imagenet_classes.txt"
      ],
      "metadata": {
        "id": "bpM1wKGiVelG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the mapping file if it's in text format\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    class_labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Assuming `imagenetv2-matched-frequency-format-val` has folders named by WNIDs\n",
        "dataset_dir = './dataset/imagenetv2-matched-frequency-format-val'\n",
        "\n",
        "# print(class_labels)\n",
        "# print(listdir(dataset_dir))\n",
        "folder_to_class = {int(id): class_labels[int(id)] for id in listdir(dataset_dir)}\n",
        "\n",
        "# # Display the mapping\n",
        "# for wnid, label in labels.items():\n",
        "#     print(f\"{wnid}: {label}\")"
      ],
      "metadata": {
        "id": "nMmJXMMUV5PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device', device)"
      ],
      "metadata": {
        "id": "N3t0qoPTTu4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(weights='default'):\n",
        "    return models.resnet50(weights) if weights == 'default' else models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)"
      ],
      "metadata": {
        "id": "Ok1Dht1TBAbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('default')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "f6Pp7xAgTrRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImagenetDataset(Dataset):\n",
        "    def __init__(self, labels, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = [] #pathe of the image + label\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        #zip the img path and the label\n",
        "        for folder_path in labels.keys():\n",
        "            # List all files in the folder and add them with the label to img_labels\n",
        "            full_path = path.join(img_dir, str(folder_path))\n",
        "            for img_file in listdir(full_path):\n",
        "                img_path = path.join(full_path, img_file)\n",
        "                if path.isfile(img_path):\n",
        "                    self.img_labels.append((img_path, folder_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path,label = self.img_labels[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "T3QuqoG4TwGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(visualization=False) -> torch.utils.data.DataLoader:\n",
        "\n",
        "    # Load data\n",
        "    preprocess_steps = [\n",
        "        transforms.Resize(256),               # Resize the shortest side to 256 pixels\n",
        "        transforms.CenterCrop(224),           # Crop to 224x224 pixels around the center\n",
        "        transforms.ToTensor(),                # Convert image to PyTorch tensor [0, 1] range\n",
        "    ]\n",
        "\n",
        "    # Conditionally add normalization for training/testing\n",
        "    if not visualization:\n",
        "        preprocess_steps.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))  # Normalize with ImageNet values\n",
        "\n",
        "    preprocess = transforms.Compose(preprocess_steps)\n",
        "    imagenet_v2_dataset = ImagenetDataset(folder_to_class, img_dir=\"./dataset/imagenetv2-matched-frequency-format-val\",transform = preprocess)\n",
        "    return imagenet_v2_dataset"
      ],
      "metadata": {
        "id": "LBLU3OV4wEWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "imagenet_v2_dataset = load_dataset(True)\n"
      ],
      "metadata": {
        "id": "L-4HRMzRYXQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize image and the corresponding label"
      ],
      "metadata": {
        "id": "cqezU-_Ooedp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_images_with_desc(image_tensors, labels, figsize=(15, 6)):\n",
        "    # Check if the number of images matches the number of titles\n",
        "    if len(image_tensors) != len(labels):\n",
        "        raise ValueError(\"The number of images must match the number of titles.\")\n",
        "\n",
        "    # Create a figure with the specified size\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Loop through the images and titles to create subplots\n",
        "    for i, (image, title) in enumerate(zip(image_tensors, labels)):\n",
        "        image = image.permute(1, 2, 0)\n",
        "        plt.subplot(1, len(image_tensors), i + 1)  # Adjust the number of columns based on the number of images\n",
        "        plt.title(title)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.tight_layout()  # Adjust the layout\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def folder_to_label(folder):\n",
        "  return folder_to_class[int(folder)]"
      ],
      "metadata": {
        "id": "GvoS_xbEgoav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "image_labels = []\n",
        "# print(len(imagenet_v2_dataset))\n",
        "for i in range(5):\n",
        "  image,folder = imagenet_v2_dataset[random.randint(0, len(imagenet_v2_dataset))]\n",
        "  images.append(image)\n",
        "  image_labels.append(folder_to_label(folder))\n",
        "\n",
        "visualize_images_with_desc(images, image_labels)\n"
      ],
      "metadata": {
        "id": "qSLNStAWge3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Perfomance Evaluation"
      ],
      "metadata": {
        "id": "8pEvMLXqpNaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagenet_v2_dataset = load_dataset(visualization=False)"
      ],
      "metadata": {
        "id": "YIt7QSCDxVYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "batch_size = 10\n",
        "# Create a DataLoader for the test set\n",
        "test_loader = DataLoader(imagenet_v2_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize counters for accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\"\"\"\n",
        "dataset need unsqueeze and squeeze\n",
        "dataloader dont\n",
        "`\"\"\"\n",
        "\n",
        "\n",
        "# Disable gradient calculation for inference\n",
        "with torch.no_grad():\n",
        "    for images, ground_truth in tqdm(test_loader):\n",
        "        # Forward pass through the model\n",
        "        images = images.to(device)\n",
        "        ground_truth = ground_truth.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        # Update the total and correct counts\n",
        "\n",
        "        total += images.size(0) #batch size\n",
        "        correct += (predicted == ground_truth).sum().item()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "XXspld61ibLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memo\n"
      ],
      "metadata": {
        "id": "wuQH6bebDpTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import AugMix, InterpolationMode, ToPILImage, ToTensor\n",
        "\n",
        "def create_augmented_batch(image, mode='augmix', n=8):\n",
        "    \"\"\"\n",
        "    Create a batch of augmented images using AugMix.\n",
        "    \"\"\"\n",
        "    image = ToPILImage()(image)\n",
        "    preaugment = transforms.Compose([\n",
        "        AugMix(severity=10, mixture_width=2),\n",
        "        transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.CenterCrop(224),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    augmentations = [preaugment(image) for _ in range(n)]\n",
        "\n",
        "    image = ToTensor()(image)\n",
        "\n",
        "    return torch.stack([image] + augmentations)"
      ],
      "metadata": {
        "id": "qt2u_B-dsjlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(img):\n",
        "    plt.imshow(img.squeeze(0).permute(1, 2, 0))\n",
        "\n",
        "def show_batch_images(batch_tensor):\n",
        "    batch_size = batch_tensor.shape[0]\n",
        "    fig, axs = plt.subplots(1, batch_size, figsize=(batch_size * 3, 3))\n",
        "\n",
        "    if batch_size == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    for i, ax in enumerate(axs):\n",
        "        img = T.ToPILImage()(batch_tensor[i])\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y1M1eDixzK8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image, test_label = next(iter(imagenet_v2_dataset))\n",
        "show_image(test_image)"
      ],
      "metadata": {
        "id": "4EnwObidxwZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = create_augmented_batch(test_image, n=8)\n",
        "show_batch_images(batch)"
      ],
      "metadata": {
        "id": "NSLi4Y0tzma9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(imagenet_v2_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "for image in tqdm(test_loader):\n",
        "    batch = create_augmented_batch(image, n=8)\n",
        "    original_model = deepcopy(model)\n",
        ""
      ],
      "metadata": {
        "id": "iR7B-XDj1tXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}